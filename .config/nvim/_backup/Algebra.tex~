\documentclass[a4paper]{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{textcomp}
\usepackage[spanish, es-tabla]{babel}
\usepackage{amsmath, amssymb}

\input{/home/jose/Documents/latex/preamble/paquetes1}
\input{/home/jose/Documents/latex/preamble/paquetesTikz}
%\input{/home/jose/latex/preamble/tikzstyle} %Estilo para las gráficas

\pgfplotsset{compat=1.16}

\usepackage[backend=bibtex,style=phys,biblabel=dot]{biblatex}
%\addbibresource{~/Documents/latex/preamble/references.bib}
\bibliography{~/Documents/latex/preamble/references.bib}

\begin{document}
    \begin{center}
       \LARGE{\textbf{Introducción a la Mecánica Cuántica}}\\ \vspace{4mm}

       \Large{Una introducción a la mecánica cuántica para mis panas del discord.}\\ \vspace{4mm}
       \large{\today} \\ \vspace{4mm}
       \large{José Antonio Quiñonero Gris}\\ \vspace{5mm}
       \Large{\textbf{Introducción al Álgebra Lineal I}}\\ \vspace{5mm}
   \end{center}

   \textbf{Resumen}: En este artículo repasaremos las bases del álgebra lineal, matrices, sus propiedades y determinantes. La idea es que el lector pueda acudir al mismo si le es necesario para el seguimiento de otros artículos sobre mecánica cuántica, además de repasar un poco de álgebra.

\section{Introducción}%
\label{sec:breve_introduccion}

He estructurado el artículo de manera que el contenido realmente importante (definiciones, propiedades, etc.) se encuentre dentro del texto destacado, de forma que todo lo que se encuentre fuera de los cuadros es, en su mayoría, demostraciones matemáticas y anotaciones. Así, el lector puede restringirse al contenido destacado y, si tiene alguna duda, puede comprobar la demostración.
\section{Matrices}
Comencemos por algunas definiciones:

\begin{bluebox}{Matriz}
	 Conjunto de $N \times M $ números ordenados en forma de tabla con $N$ filas y $M$ columnas.
	\[
		\boldsymbol{\mathrm{A}} = \begin{bmatrix}
			A_{11} & A_{12} & \dots & \ldots & A_{1M} \\
			A_{21} & A_{22} & \ldots & \ldots & A_{2M} \\
			\vdots & \vdots  & \vdots & \vdots & \vdots \\
			A_{N1} & A_{N2} & \ldots & \ldots & A_{NM} \\
		\end{bmatrix}
	\]
\end{bluebox}

\begin{bluebox}{Elemento de una matriz}
	 Simbolizamos el elemento de una matriz con dos subíndices. Sea $A_{ij} = \left( \boldsymbol{\mathrm{A}} \right)_{ij}$, el elemento $\left( i,j \right) $ se localiza en la fila $i$ y la columna $j$.
\end{bluebox}

\begin{bluebox}{Matriz cuadrada}
Matriz en la que $M = N$.
\end{bluebox}

\begin{bluebox}{Producto de matrices}
 En general, el producto de matrices no es conmutativa, así:
\[
\boldsymbol{\mathrm{A}} \boldsymbol{\mathrm{B}} \not= \boldsymbol{\mathrm{B}} \boldsymbol{\mathrm{A}}
\]
\end{bluebox}

Se dice que dos matrices son compatibles para el producto si el número de columnas de la primera coincide con el número de filas de la segunda, de forma que la matriz resultante tendrá el número de filas que presenta la primera matriz y el número de columnas de la segunda matriz. Siendo $\boldsymbol{\mathrm{A}}\left( N \times M \right) $, una matriz $\boldsymbol{\mathrm{A}}$ de $N$ filas y $M$ columnas, el producto de esta con otra matriz $\boldsymbol{\mathrm{B}}\left( M \times P \right) $ da una matriz resultante $\boldsymbol{\mathrm{C}}\left( N \times P \right) $ :
\[
	\boldsymbol{\mathrm{A}}\left( N \times M \right) \otimes \boldsymbol{\mathrm{B}}\left( M \times P \right) = \boldsymbol{\mathrm{C}} \left( N \times P \right)
\]
Donde simbolizamos con $\otimes$ el producto de matrices.\\

Sean $\boldsymbol{\mathrm{A}}$ y $\boldsymbol{\mathrm{B}}$ dos matrices cuadradas $2 \times 2$
\[
\boldsymbol{\mathrm{A}} = \begin{bmatrix}
	a_{11} & a_{12} \\
	a_{21} & a_{22}
\end{bmatrix} \qquad
\boldsymbol{\mathrm{B}} = \begin{bmatrix}
	b_{11} & b_{12} \\
	b_{21} & b_{22}
\end{bmatrix}
\]
Calculamos el producto como
\begin{align*}
	\boldsymbol{\mathrm{A}} \otimes \boldsymbol{\mathrm{B}} &= \begin{bmatrix}
	a_{11}b_{11} + a_{12}b_{21} & a_{11}b_{12} + a_{12}b_{22}\\
	a_{21}b_{12} + a_{22}b_{21} & a_{21}b_{12} + a_{22}b_{22}
	\end{bmatrix} \\
		\boldsymbol{\mathrm{B}} \otimes \boldsymbol{\mathrm{A}} &= \begin{bmatrix}
			b_{11}a_{11} + b_{12}a_{21} & b_{11}a_{12} + b_{12}a_{22}\\
			b_{21}a_{11} + b_{22}a_{21} & b_{21}a_{12} + b_{22}a_{22}
	\end{bmatrix}
\end{align*}
El producto de escalares $\left( a_{ij} \times b_{ij} \right) $ es conmutativo, por lo que vemos que se cumple $\boldsymbol{\mathrm{A}}\boldsymbol{\mathrm{B}} \not= \boldsymbol{\mathrm{B}}\boldsymbol{\mathrm{A}}$. Además, se puede ver que la matriz resultante del producto también es $2 \times 2$.

\begin{bluebox}{Matriz columna}
 También llamada \textbf{vector}, es aquella matriz en la que el número de columnas es uno, $M = 1$. El producto de una matriz $\boldsymbol{\mathrm{A}}(N \times  M)$ por un vector $\boldsymbol{\mathrm{a}}\left( M \times 1 \right) $ resulta en un vector $\boldsymbol{\mathrm{b}}\left( N\times 1 \right) $
	\[
		\boldsymbol{\mathrm{A}}\left( N \times M \right) \otimes \boldsymbol{\mathrm{a}}\left( M \times 1 \right) = \boldsymbol{\mathrm{b}}(N \times 1)
	\]
\end{bluebox}

Si tomamos ahora como matriz $\boldsymbol{\mathrm{A}}\left( 2\times 2 \right) $ y vector $\boldsymbol{\mathrm{a}}\left( 2\times 1 \right) $
\[
\boldsymbol{\mathrm{A}} = \begin{bmatrix}
	A_{11} & A_{12}\\
	A_{21} & A_{22}
\end{bmatrix}
\qquad
\boldsymbol{\mathrm{a}} = \begin{bmatrix}
a_{1}\\
a_{2}
\end{bmatrix}
\]
y realizamos el producto
\[
	\boldsymbol{\mathrm{A}}(2\times 1) \otimes \boldsymbol{\mathrm{a}}\left( 2\times 1 \right) = \begin{bmatrix}
		A_{11}a_{1} + A_{12}a_{2}\\
		A_{21}a_{1} + A_{22}a_{2}
	\end{bmatrix} = \boldsymbol{\mathrm{b}}\left( 2\times 1 \right)
\]
vemos que los elementos $b_i$ del vector resultante $\boldsymbol{\mathrm{b}}$ se pueden calcular como
\begin{empheq}[box={\mymathbox}]{equation*}
	b_i = \sum_{j=1}^{M} A_{ij} a_j \qquad i = 1,\, 2,\, \ldots,\, N
\end{empheq}

\begin{bluebox}{Matriz transpuesta}
  Dada una matriz $\boldsymbol{\mathrm{A}}$, se dice que la matriz $\boldsymbol{\mathrm{\widetilde{A}}}$ es su matriz transpuesta si se cumple que el elemento $\left( i,j \right) $ de $\boldsymbol{\mathrm{A}}$ es igual al elemento $\left( j,i \right) $ de $\boldsymbol{\mathrm{\widetilde{A}}}$, es decir, si
	\[
		\left( \boldsymbol{\mathrm{\widetilde{A}}} \right)_{ji} = \left( \boldsymbol{\mathrm{A}} \right)_{ij}
	\]
\end{bluebox}

La transpuesta de una matriz es aquella en la que se han intercambiado las filas por las columnas de la matriz original. Sea una matriz
\[
\boldsymbol{\mathrm{A}} = \begin{bmatrix}
	a_{11} & a_{12}\\
	a_{21} & a_{22}
\end{bmatrix}
\]
los elementos de la primera fila pasarán a formar la primera columna, y los elementos de la segunda fila, la segunda columna, de forma que
\[
\boldsymbol{\mathrm{A}} = \begin{bmatrix}
	a_{11} & a_{12}\\
	a_{21} & a_{22}
\end{bmatrix}  \to
\boldsymbol{\mathrm{\widetilde{A}}} = \begin{bmatrix}
	a_{11} & a_{21}\\
	a_{12} & a_{22}
\end{bmatrix}
\]
\begin{bluebox}{Matriz adjunta}
 Dada una matriz $\boldsymbol{\mathrm{A}}$, decimos que la matriz $\boldsymbol{\mathrm{A^{\dagger}}}$ es su matriz adjunta si es su matriz transpuesta y compleja conjugada
\[
\boldsymbol{\mathrm{A^{\dagger}}} = \boldsymbol{\mathrm{\widetilde{A}^*}}
\]
Los elementos de dichas matrices cumplen la siguiente relación
\[
	\left( \boldsymbol{\mathrm{A^{\dagger}}} \right)_{ij} = \left( \boldsymbol{\mathrm{A}} \right)_{ji}^*
\]
Siendo $\left( \boldsymbol{\mathrm{A^{\dagger}}} \right)_{ij}$ los elementos de la matriz adjunta, también podremos calcularlos (especialmente para matrices reales) según
\[
	\left( \boldsymbol{\mathrm{A^{\dagger}}} \right)_{ij} = \left( -1 \right)^{i + j} \mathrm{det}\, \boldsymbol{\mathrm{A}}_{ij}
\]
donde $\boldsymbol{\mathrm{A}}_{ij}$ es la \textbf{menor-$\left( \boldsymbol{i,j} \right)$} de la matriz $\boldsymbol{\mathrm{A}}$ \footnote{Básicamente, es la matriz resultante de eliminar la fila $i$ y la columna $j$ de la matriz $\boldsymbol{\mathrm{A}}$. (Ver la sección \ref{sec:determinantes})}.
\end{bluebox}

Dada una matriz compleja
\[
\boldsymbol{\mathrm{A}} = \begin{bmatrix}
	a + bi & c - di\\
	e - fi & g + hi
\end{bmatrix}
\]
calculamos la compleja conjugada teniendo en cuenta que $i^* = -i$, de forma que $|i|^2 = i^*i = -i^2=-\left( \sqrt{-1}  \right)^2 = 1$. Así
\[
\boldsymbol{\mathrm{A^*}} = \begin{bmatrix}
	a - bi & c + di\\
	e + fi & g - hi
\end{bmatrix}
\]
y su matriz transpuesta será
\[
\boldsymbol{\mathrm{\widetilde{A}^*}} = \boldsymbol{\mathrm{A^{\dagger}}} = \begin{bmatrix}
	a - bi &  e + fi \\
	c + di & g - hi
\end{bmatrix}
\]
Podemos ver que se cumple la condición $\left( \boldsymbol{\mathrm{A^{\dagger}}} \right)_{ij} = \left( \boldsymbol{\mathrm{A}} \right)_{ji}^*$.\\

Si la matriz $\boldsymbol{\mathrm{A}}$ es real (no tiene componente imaginaria), su compleja conjugada es igual a la matriz $\boldsymbol{\mathrm{A}} = \boldsymbol{\mathrm{A^*}}$, de forma que
\begin{empheq}[box={\mymathbox}]{equation*}
	\text{Si \textbf{A} es real} \implies \boldsymbol{\mathrm{A^{\dagger}}} = \boldsymbol{\mathrm{\widetilde{A}}}
\end{empheq}

Para el producto de dos matrices se cumple
\begin{empheq}[box={\mymathbox}]{equation*}
	\left( \boldsymbol{\mathrm{A}}\boldsymbol{\mathrm{B}} \right)^{\dagger} = \boldsymbol{\mathrm{B^{\dagger}}} \boldsymbol{\mathrm{A^{\dagger}}}
\end{empheq}
Dadas dos matrices reales (por simplicidad, obviamente también se podría demostrar con matrices complejas)
\[
\boldsymbol{\mathrm{A}} = \begin{bmatrix}
	a_{11} & a_{12} \\
	a_{21} & a_{22}
\end{bmatrix} \qquad
\boldsymbol{\mathrm{B}} = \begin{bmatrix}
	b_{11} & b_{12} \\
	b_{21} & b_{22}
\end{bmatrix}
\]
Sabemos que, al ser reales,  $\boldsymbol{\mathrm{A^{\dagger}}} = \boldsymbol{\mathrm{\widetilde{A}}}$, $\boldsymbol{\mathrm{B^{\dagger}}} = \boldsymbol{\mathrm{\widetilde{B}}}$ y $\boldsymbol{\mathrm{\left( AB \right)^{\dagger}}} = \left( \boldsymbol{\mathrm{\widetilde{AB}}} \right)$. Por tanto, para este caso, podemos escribir la condición anterior como $\left( \boldsymbol{\mathrm{\widetilde{AB}}} \right) = \boldsymbol{\mathrm{\widetilde{B}}} \boldsymbol{\mathrm{\widetilde{A}}}$. La comprobamos haciendo
\begin{align*}
	\boldsymbol{\mathrm{AB}} &= \begin{bmatrix}
	a_{11}b_{11} + a_{12}b_{21} & a_{11}b_{12} + a_{12}b_{22}\\
	a_{21}b_{11} + a_{22}b_{21} & a_{21}b_{12} + a_{22}b_{22}
	\end{bmatrix}\\
		\boldsymbol{\mathrm{\widetilde{AB}}} &= \begin{bmatrix}
	a_{11}b_{11} + a_{12}b_{21} & a_{21}b_{11} + a_{22}b_{21} \\
	 a_{11}b_{12} + a_{12}b_{22} & a_{21}b_{12} + a_{22}b_{22}
	\end{bmatrix}
\end{align*}
y
\begin{align*}
	&\boldsymbol{\mathrm{\widetilde{B}}} = \begin{bmatrix}
	b_{11} & b_{21} \\
	b_{12} & b_{22}
\end{bmatrix} \quad \boldsymbol{\mathrm{\widetilde{A}}} = \begin{bmatrix}
	a_{11} & a_{21} \\
	a_{12} & a_{22}
\end{bmatrix} \\
\boldsymbol{\mathrm{\widetilde{B}}} \boldsymbol{\mathrm{\widetilde{A}}} &= \begin{bmatrix}
	a_{11}b_{11} + a_{12}b_{21} & a_{21}b_{12} + a_{22}b_{21} \\
	 a_{11}b_{12} + a_{12}b_{22} & a_{21}b_{12} + a_{22}b_{22}
	\end{bmatrix}
\end{align*}
vemos que se cumple la condición $\left( \boldsymbol{\mathrm{\widetilde{AB}}} \right) = \boldsymbol{\mathrm{\widetilde{B}}} \boldsymbol{\mathrm{\widetilde{A}}}$.

\begin{bluebox}{Adjunto de un vector}
 Dado un vector $\boldsymbol{\mathrm{a}}$ se define como su vector adjunto $\boldsymbol{\mathrm{a^{\dagger}}}$ el vector fila
	\[
	\boldsymbol{\mathrm{a}} = \begin{bmatrix}
	a_1 \\
	a_2 \\
	\vdots \\
	a_N
	\end{bmatrix} \to \boldsymbol{\mathrm{a^{\dagger}}} = \begin{bmatrix}
		a_1^* & a_2^* & \ldots & a_N^*
	\end{bmatrix}
	\]
	En el caso de un operador $\mathcal{A}$ operando sobre un vector $\boldsymbol{\mathrm{a}}$ se cumple que
	\[
		\mathcal{A}\boldsymbol{\mathrm{a}} = \boldsymbol{\mathrm{b}} \implies \boldsymbol{\mathrm{b^{\dagger}}} = \left( \mathcal{A}\boldsymbol{\mathrm{a}} \right)^{\dagger} = \boldsymbol{\mathrm{a^{\dagger}}} \mathcal{A^{\dagger}}
	\]
	es decir, que
	\[
		b_i^* = \sum_{j} a_j^* \left( \boldsymbol{\mathrm{A^{\dagger}}} \right)_{ji}= \left( \sum_{j} a_j A_{ij} \right)^*
	\]
\end{bluebox}

\begin{bluebox}{Matriz unidad}
 Es aquella matriz que cumple $1\boldsymbol{\mathrm{A}} = \boldsymbol{\mathrm{A}}1 = \boldsymbol{\mathrm{A}}$, de forma que
\[
	\left( 1 \right)_{ij} = \delta_{ij}
\]

\[
\delta = \begin{bmatrix}
	1 & 0 & 0 & \ldots & 0\\
	0 & 1 & 0 & \ldots & 0\\
	0 & 0 & 1 & \ldots & 0\\
	\vdots  & \vdots  & \vdots  & \ddots & \vdots \\
	0 & 0 & 0 & \ldots & 1
\end{bmatrix}
\]
Representaremos la matriz unidad con la letra griega \textit{delta} $\delta$, y cada elemento de la matriz unidad como $\delta_{ij}$.
\end{bluebox}

La matriz unidad nos sirve para convertir cualquier matriz en una matriz diagonal ya que, si nuestra matriz es de la forma
\[
\boldsymbol{\mathrm{A}} = \begin{bmatrix}
	a_{11} & a_{12}\\
	a_{21} & a_{22}
\end{bmatrix}
\]
tenemos que $A_{11} = a_{11}$ y $\delta_{12} = 0$, por lo que $A_{12} = A_{11}\delta_{12} = 0$, y si lo hacemos para cada elemento encontraremos que solo se mantienen los elementos de la diagonal principal, y el resto, ceros
\[
\boldsymbol{\mathrm{A}} = \begin{bmatrix}
	a_{11} & 0\\
	0 & a_{22}
\end{bmatrix}
\]

\begin{bluebox}{Traza de una matriz}
 Suma de los elementos de la diagonal
\[
	\mathrm{tr}\left( \boldsymbol{\mathrm{A}} \right) = \sum_i A_{ii}
\]
\end{bluebox}

Por ejemplo
\[
\boldsymbol{\mathrm{A}} = \begin{bmatrix}
	2 & 0\\
	0 & 2
\end{bmatrix} \to \mathrm{tr}\left( \boldsymbol{\mathrm{A}} \right) = 4
\]
\begin{bluebox}{Inversa de una matriz}
 Para toda matriz $\boldsymbol{\mathrm{A}}$ existe su inversa $\boldsymbol{\mathrm{A^{-1}}}$ si el producto de la matriz por su inversa da la matriz unidad y/o un determinante no nulo
\[
\forall \boldsymbol{\mathrm{A}} \implies \exists \boldsymbol{\mathrm{A^{-1}}} / \boldsymbol{\mathrm{A}} \boldsymbol{\mathrm{A^{-1}}} = \boldsymbol{\mathrm{A^{-1}}} \boldsymbol{\mathrm{A}} = 1
\]
Si el determinante de $\boldsymbol{\mathrm{A}}$ es nulo, $\boldsymbol{\mathrm{A}}$ no tiene inversa y se dice que es \textbf{irregular} o \textbf{singular}.\\
Si el determinante de $\boldsymbol{\mathrm{A}}$ es no nulo, $\boldsymbol{\mathrm{A}}$ tiene inversa y se dice que es \textbf{inversible} o \textbf{regular}.\\

Podemos encontrar la inversa de una matriz regular mediante el método de \textbf{inversa de una matriz por adjunción}, el cual se basa en el hecho de que su determinante es no nulo
\[
	\boldsymbol{\mathrm{A^{-1}}} = \frac{\boldsymbol{\mathrm{\widetilde{A}^{\dagger}}}}{|\boldsymbol{\mathrm{A}}|}
\]
Para hallar la inversa de la matriz $\boldsymbol{\mathrm{A}}$, tendremos que hacer el producto de la inversa de su determinante\footnote{Recuerde que el determinante de una matriz es un escalar, mientras que una matriz no lo es. Por esta razón, no piense en hallar la inversa de una matriz como $1 / \boldsymbol{\mathrm{A}}$, ya que estaría dividiendo un escalar $(1)$ entre una matriz. Lo que usted esta buscando en ese caso es el producto de $1$ por la matriz inversa. Sin embargo, el determinante de una matriz da un escalar, por lo que podemos realizar la inversa de un determinante sin problema.} por la transpuesta de la matriz adjunta.
\end{bluebox}

Si
\[
\boldsymbol{\mathrm{A}} = \begin{bmatrix}
	a_{11} & a_{12}\\
	a_{21} & a_{22}
\end{bmatrix}
\]
entonces $\boldsymbol{\mathrm{A}}$ es invertible si $a_{11}a_{22} - a_{21}a_{12} \not= 0$, en cuyo caso
\[
\boldsymbol{\mathrm{A^{-1}}} = \frac{1}{a_{11}a_{22} - a_{21}a_{12}} \begin{bmatrix}
	a_{22} & -a_{12}\\
	-a_{21} & a_{11}
\end{bmatrix}
\]

\begin{bluebox}{Matriz unitaria}
 Aquella para la que $\boldsymbol{\mathrm{A^{\dagger}}} = \boldsymbol{\mathrm{A^{-1}}}$, por tanto $\boldsymbol{\mathrm{A}} \boldsymbol{\mathrm{A^{\dagger}}} = \boldsymbol{\mathrm{A^{\dagger}}} \boldsymbol{\mathrm{A}} = 1$\footnote{Esta formulación tiene especial importancia en mecánica cuántica, donde representamos con la \textit{daga} $\dagger$ la matriz adjunta Hermítica.}\\
	La expresión general de una matriz unitaria $2 \times 2$ es
	\[
	\boldsymbol{\mathrm{U}} = \begin{bmatrix}
		a & b\\
		-e^{i \varphi} b^* & e^{i \varphi} a^*
	\end{bmatrix}, \qquad |a|^2 + ¹|b|^2 = 1
	\]
	Si $\boldsymbol{\mathrm{A}}$ es real, se dice entonces que es \textbf{ortogonal}, ya que $\boldsymbol{\mathrm{\widetilde{A}}} = \boldsymbol{\mathrm{A^{-1}}}$ y, por tanto, $\boldsymbol{\mathrm{A}} \boldsymbol{\mathrm{\widetilde{A}}} = \boldsymbol{\mathrm{\widetilde{A}}} \boldsymbol{\mathrm{A}} = 1$.
\end{bluebox}

\begin{bluebox}{Matriz Hermítica}
 Se dice que una matriz es Hermítica si $\boldsymbol{\mathrm{A}} = \boldsymbol{\mathrm{A^{\dagger}}}$, es decir, si $\left( \boldsymbol{\mathrm{A^{\dagger}}} \right)_{ij} = \left( \boldsymbol{\mathrm{A}} \right)_{ij}$.\\

	Por definición, $\left( \boldsymbol{\mathrm{A^{\dagger}}} \right)_{ij} = \left( \boldsymbol{\mathrm{A}} \right)_{ji}^*$, por lo que $\left( \boldsymbol{\mathrm{A}} \right)_{ij} = \left( \boldsymbol{\mathrm{A}} \right)_{ji}^*$. Por tanto, todos los elementos $A_{ii}$ son números reales ya que, por la definición de la matriz adjunta, $\left( \boldsymbol{\mathrm{A^{\dagger}}} \right)_{ii} = \left( \boldsymbol{\mathrm{A}} \right)_{ii}^* = \left( \boldsymbol{\mathrm{A}} \right)_{ii}$. Si $\boldsymbol{\mathrm{A}}$ es real, entonces $\boldsymbol{\mathrm{A^{\dagger}}} = \boldsymbol{\mathrm{\widetilde{A}}} = \boldsymbol{\mathrm{A}}$ y se dice que $\boldsymbol{\mathrm{A}}$ es \textbf{simétrica}.
\end{bluebox}

Para
\[
\boldsymbol{\mathrm{A}} = \begin{bmatrix}
	3 & 1\\
	1 & 3
\end{bmatrix} \to \boldsymbol{\mathrm{A^{\dagger}}} = \begin{bmatrix}
	3 & 1\\
	1 & 3
\end{bmatrix}
\]
Tenemos que $\boldsymbol{\mathrm{A}} = \boldsymbol{\mathrm{A^{\dagger}}}$ y $\left( \boldsymbol{\mathrm{A}} \right)_{ij} = \left( \boldsymbol{\mathrm{A^{\dagger}}} \right)_{ij}$. Al ser real, comprobamos también que $\left( \boldsymbol{\mathrm{A}} \right)_{ij} = \left( \boldsymbol{\mathrm{A}} \right)_{ji}^*$ y
\[
\boldsymbol{\mathrm{A^{\dagger}}} = \begin{bmatrix}
	3 & 1\\
	1 & 3
\end{bmatrix} = \boldsymbol{\mathrm{\widetilde{A}}} = \begin{bmatrix}
	3 & 1\\
	1 & 3
\end{bmatrix} = \boldsymbol{\mathrm{A}}
\]
Por lo que concluimos que $\boldsymbol{\mathrm{A}}$ es simétrica.

\subsection{Propiedades de las matrices}

A continuación, vamos a repasar algunas de las propiedades más importantes de las matrices:

\begin{bluebox}{Propiedades de las matrices}
\begin{enumerate}
	\item $\mathrm{tr}\left( \boldsymbol{\mathrm{AB}} \right) = \mathrm{tr}\left( \boldsymbol{\mathrm{BA}} \right)$
	\item $\left(  \boldsymbol{\mathrm{AB}} \right)^{-1} = \boldsymbol{\mathrm{B^{-1}}} \boldsymbol{\mathrm{A^{-1}}}$
	\item Si la matriz $\boldsymbol{\mathrm{U}}$ es unitaria y $\boldsymbol{\mathrm{B}} =  \boldsymbol{\mathrm{U^{\dagger}}} \boldsymbol{\mathrm{A}} \boldsymbol{\mathrm{U}}$, entonces $\boldsymbol{\mathrm{A}} = \boldsymbol{\mathrm{U}} \boldsymbol{\mathrm{B}} \boldsymbol{\mathrm{U^{\dagger}}}$
	\item Si $\boldsymbol{\mathrm{C}} = \boldsymbol{\mathrm{AB}}$ y $\boldsymbol{\mathrm{A}},\ \boldsymbol{\mathrm{B}},\ \boldsymbol{\mathrm{C}}$ son Hermíticas, entonces $\boldsymbol{\mathrm{AB}} = \boldsymbol{\mathrm{BA}}$
	\item Si $\boldsymbol{\mathrm{A}}$ es Hermítica, entonces $\boldsymbol{\mathrm{A^{-1}}}$, si existe, también lo es
\end{enumerate}
\end{bluebox}

Tomando $\boldsymbol{\mathrm{A}}$ y $\boldsymbol{\mathrm{B}}$ como dos matrices ejemplo para realizar las demostraciones
\[
\boldsymbol{\mathrm{A}} = \begin{bmatrix}
	1 & 2\\
	3 & 4
\end{bmatrix} \qquad
\boldsymbol{\mathrm{B}} = \begin{bmatrix}
	5 & 6\\
	7 & 8
\end{bmatrix}
\]
\begin{enumerate}
		\item $\mathrm{tr}\left( \boldsymbol{\mathrm{AB}} \right) = \mathrm{tr}\left( \boldsymbol{\mathrm{BA}} \right)$
			\begin{align*}
	\boldsymbol{\mathrm{AB}} = \begin{bmatrix}
		19 & 22\\
		43 & 50
	\end{bmatrix} &\to \mathrm{tr} \left( \boldsymbol{\mathrm{AB}} \right) = 69\\
		\boldsymbol{\mathrm{BA}} = \begin{bmatrix}
			23 & 43\\
			31 & 46
		\end{bmatrix} &\to \mathrm{tr} \left( \boldsymbol{\mathrm{BA}} \right) = 69
\end{align*}

\[
	\mathrm{tr} \left( \boldsymbol{\mathrm{AB}} \right) = \mathrm{tr} \left( \boldsymbol{\mathrm{BA}} \right)
\]
\item $\left(  \boldsymbol{\mathrm{AB}} \right)^{-1} = \boldsymbol{\mathrm{B^{-1}}} \boldsymbol{\mathrm{A^{-1}}}$
	\[
	\left( \boldsymbol{\mathrm{AB}} \right)^{-1} = \frac{1}{4}\begin{bmatrix}
		50 & -22\\
		-43 & 19
	\end{bmatrix}
\]

\[
\boldsymbol{\mathrm{B^{-1}}} \boldsymbol{\mathrm{A^{-1}}} = \frac{1}{-2} \begin{bmatrix}
	8 & -6\\
	-7 & 5
\end{bmatrix} \frac{1}{-2} \begin{bmatrix}
	4 & -2\\
	-3 & 1
\end{bmatrix} = \frac{1}{4} \begin{bmatrix}
	50 & -22\\
	-43 & 19
\end{bmatrix}
\]

\[
\left(  \boldsymbol{\mathrm{AB}} \right)^{-1} = \boldsymbol{\mathrm{B^{-1}}} \boldsymbol{\mathrm{A^{-1}}}
\]
\item Si la matriz $\boldsymbol{\mathrm{U}}$ es unitaria y $\boldsymbol{\mathrm{B}} =  \boldsymbol{\mathrm{U^{\dagger}}} \boldsymbol{\mathrm{A}} \boldsymbol{\mathrm{U}}$, entonces $\boldsymbol{\mathrm{A}} = \boldsymbol{\mathrm{U}} \boldsymbol{\mathrm{B}} \boldsymbol{\mathrm{U^{\dagger}}}$. \\

Siendo
\[
\boldsymbol{\mathrm{U}} = \begin{bmatrix}
	1 & 0\\
	0 & 1
\end{bmatrix} \to \boldsymbol{\mathrm{U^{\dagger}}} = \boldsymbol{\mathrm{U}}
\]

\[
\boldsymbol{\mathrm{B}} = \boldsymbol{\mathrm{U^{\dagger}}} \boldsymbol{\mathrm{A}} \boldsymbol{\mathrm{U}} = \begin{bmatrix}
	1 & 0\\
	0 & 1
\end{bmatrix} \begin{bmatrix}
	1 & 2\\
	3 & 4
\end{bmatrix} \begin{bmatrix}
	1 & 0\\
	0 & 1
\end{bmatrix} = \begin{bmatrix}
	1 & 2\\
	3 & 4
\end{bmatrix}
\]

\[
\boldsymbol{\mathrm{UB}} \boldsymbol{\mathrm{U^{\dagger}}} = \begin{bmatrix}
	1 & 0\\
	0 & 1
\end{bmatrix} \begin{bmatrix}
	1 & 2\\
	3 & 4
\end{bmatrix} \begin{bmatrix}
	1 & 0\\
	0 & 1
\end{bmatrix} = \begin{bmatrix}
	1 & 2\\
	3 & 4
\end{bmatrix} = \boldsymbol{\mathrm{A}}
\]

\item Si $\boldsymbol{\mathrm{C}} = \boldsymbol{\mathrm{AB}}$ y $\boldsymbol{\mathrm{A}},\ \boldsymbol{\mathrm{B}},\ \boldsymbol{\mathrm{C}}$ son Hermíticas, entonces $\boldsymbol{\mathrm{AB}} = \boldsymbol{\mathrm{BA}}$.\\

Que $\boldsymbol{\mathrm{A}},\ \boldsymbol{\mathrm{B}}$ y $\boldsymbol{\mathrm{C}}$ sean Hermíticas implica que
\begin{align*}
	\boldsymbol{\mathrm{A}} &= \boldsymbol{\mathrm{A^{\dagger}}}\\
	\boldsymbol{\mathrm{B}} &= \boldsymbol{\mathrm{B^{\dagger}}}\\
	\boldsymbol{\mathrm{C}} &= \boldsymbol{\mathrm{C^{\dagger}}}
\end{align*}

Siendo, para este caso (para que sean Hermíticas)
\[
\boldsymbol{\mathrm{A}} = \begin{bmatrix}
	1 & 2\\
	2 & 1
\end{bmatrix} \qquad \boldsymbol{\mathrm{B}} = \begin{bmatrix}
	3 & 4\\
	4 & 3
\end{bmatrix}
\]
Tenemos que
\[
\boldsymbol{\mathrm{C}} = \boldsymbol{\mathrm{AB}} = \begin{bmatrix}
	11 & 10\\
	10 & 11
\end{bmatrix} \to \boldsymbol{\mathrm{C}} = \boldsymbol{\mathrm{C^{\dagger}}}
\]
entonces
\[
\boldsymbol{\mathrm{BA}} = \begin{bmatrix}
	11 & 10\\
	10 & 11
\end{bmatrix} = \boldsymbol{\mathrm{AB}} \implies \boldsymbol{\mathrm{AB}} = \boldsymbol{\mathrm{BA}}
\]

\item Si $\boldsymbol{\mathrm{A}}$ es Hermítica, entonces $\boldsymbol{\mathrm{A^{-1}}}$, si existe, también lo es.\\

	Para la matriz
\[
\boldsymbol{\mathrm{A}} = \begin{bmatrix}
	1 & 2\\
	2 & 1
\end{bmatrix} \implies \boldsymbol{\mathrm{A}} = \boldsymbol{\mathrm{A^{\dagger}}}
\]
hallamos su inversa
\[
\boldsymbol{\mathrm{A^{-1}}} = -\frac{1}{3} \begin{bmatrix}
	1 & -2\\
	-2 & 1
\end{bmatrix} \to \left( \boldsymbol{\mathrm{A^{-1}}} \right)^{\dagger} = -\frac{1}{3}\begin{bmatrix}
	1 & -2\\
	-2 & 1
\end{bmatrix} \implies \boldsymbol{\mathrm{A^{-1}}} = \left( \boldsymbol{\mathrm{A^{-1}}} \right)^{\dagger}
\]
\end{enumerate}

\section{Determinantes}%
\label{sec:determinantes}

Para toda matriz cuadrada, es posible definir su determinante, que se trata de un escalar, según
\begin{empheq}[box={\mymathbox}]{equation*}
	\boldsymbol{\mathrm{A}} \to \mathrm{det}\left( \boldsymbol{\mathrm{A}} \right) = |\boldsymbol{\mathrm{A}}| = \sum_{i=1}^{N!} \left( -1 \right)^{p_i} \hat{\mathcal{P}}_i \left\{ A_{11},\, A_{22},\, \ldots,\, A_{N N} \right\}
\end{empheq}
donde $\hat{\mathcal{P}}_i$ es un operador capaz de generar una de las $N!$ permutaciones entre los $N$ índices de columna del conjunto de números a su derecha a partir de una ordenación preestablecida o canónica y $p_i$ es el número de intercambio de dos índices de columna de números contiguos necesario para resistituir el orden canónico.\\

Las permutaciones de una matriz $N \times  N$ las hallamos designando una matriz para cada combinación de sus elementos como ceros y unos, de forma que solo encontremos un 1 por cada fila y columna.\\
De esta forma, para una matriz $2 \times 2$, $N = 2 = N!$, por lo que hay dos permutaciones posibles
\[
P_1 = \begin{bmatrix}
	1 & 0\\
	0 & 1
\end{bmatrix} \qquad P_2 = \begin{bmatrix}
	0 & 1\\
	1 & 0
\end{bmatrix}
\]
en las que se puede ver que solo encontramos un 1 en cada fila y columna.\\

Si calculamos su determinante vemos que $|P_1| = 1$ y $|P_2| = -1$.\\

Clasificaremos las permutaciones como \textbf{pares} cuando su determinante es igual a 1, y como \textbf{impares} cuando su determinante es igual a $-1$.\\

A partir de esta definición formal, calcularemos el determinante de una matriz $2 \times 2$ según:
	\begin{itemize}
		\item $2\times 2$
		\[
		\boldsymbol{\mathrm{A}} = \begin{bmatrix}
			a_{11} & a_{12}\\
			a_{21} & a_{22}
		\end{bmatrix} \to |\boldsymbol{\mathrm{A}}| = a_{11}a_{22} - a_{21}a_{12}
		\]
Y el de una matriz $3\times 3$ según:

		\item $3\times 3$
		\[
		\boldsymbol{\mathrm{A}} = \begin{bmatrix}
			a_{11} & a_{12} & a_{13}\\
			a_{21} & a_{22} & a_{23}\\
			a_{31} & a_{32} & a_{33}
		\end{bmatrix}
		\]
		\[
		|\boldsymbol{\mathrm{A}}| = a_{11} \begin{vmatrix} a_{22} & a_{23}\\
		a_{32} & a_{33}\end{vmatrix} - a_{12} \begin{vmatrix} a_{21} & a_{23}\\
		a_{31} & a_{33}\end{vmatrix} + a_{13} \begin{vmatrix} a_{21} & a_{22}\\
		a_{31} & a_{32}\end{vmatrix}
		\]
\end{itemize}
Para calcular el determinante de una matriz $3\times 3$, tenemos que calcular $3$ determinantes $2\times 2$. Para cada matriz $\boldsymbol{\mathrm{A}}$ cuadrada, llamamos \textbf{menor-$\left( \boldsymbol{i,j} \right)$} de $\boldsymbol{\mathrm{A}}$ al $\mathrm{det}\, \boldsymbol{A}_{ij}$.\\

También podemos calcular el determinante de una matriz $\boldsymbol{\mathrm{A}}\left( 3 \times 3 \right)$ según:
\begin{align*}
	\mathrm{det}\, \boldsymbol{\mathrm{A}} &= a_{11}\, \mathrm{det}\, \boldsymbol{A}_{11} - a_{12}\, \mathrm{det}\, \boldsymbol{A}_{12} + a_{13}\, \mathrm{det}\, \boldsymbol{A}_{13}\\
	&= \sum_{j=1}^{3} \left( -1 \right)^{1 + j} a_{1j}\, \mathrm{det}\, \boldsymbol{A}_{1j}
\end{align*}

\begin{bluebox}{Determinantes}
La definición de determinante es recursiva, por lo que podemos definir el determinante de una matriz $n\times n$ con $n\ge 2$ según:
\begin{align*}
	\mathrm{det}\, \boldsymbol{\mathrm{A}} = |\boldsymbol{\mathrm{A}}| &= a_{11}\, \mathrm{det}\, \boldsymbol{A}_{11} - a_{12}\, \mathrm{det}\, \boldsymbol{A}_{12} + \ldots + \left( -1 \right)^{1 + n} a_{1n}\, \mathrm{det}\, \boldsymbol{A}_{1n}\\
	&= \sum_{j=1}^{n} \left( -1 \right)^{1 + j} a_{1j}\, \mathrm{det}\, \boldsymbol{A}_{1j}
\end{align*}
Para combinar un menor con su signo más o menos, definimos el \textbf{cofactor-$\left( \boldsymbol{i,j} \right)$} de $\boldsymbol{\mathrm{A}}$ como:
\[
	\boldsymbol{C}_{ij} = \left( -1 \right)^{i + j} \mathrm{det}\, \boldsymbol{A}_{ij}
\]
con lo que la definición anterior queda como
\[
\mathrm{det}\, \boldsymbol{\mathrm{A}} = \sum_{j=1}^{n} a_{1j}\, \boldsymbol{A}_{1j}
\]
que se conoce como \textit{expansión por cofactores a lo largo del primero renglón}.
\end{bluebox}

A través de esta definición llegamos al \textit{teorema de expansión de Laplace}:
\begin{bluebox}{Teorema de expansión de Laplace}
El determinante de una matriz $\boldsymbol{\mathrm{A}}$ de $n\times n$, donde $n\ge 2$, puede calcularse como:
\begin{itemize}
	\item Expansión por cofactores a lo largo del $i$-ésimo renglón
		\begin{align*}
	\mathrm{det}\, \boldsymbol{\mathrm{A}} &= a_{i1}\, \boldsymbol{C}_{i1} + a_{i2}\, \boldsymbol{C}_{i2} + \ldots + a_{in} \boldsymbol{C}_{in} \\
	&= \sum_{j=1}^{n} a_{ij} \boldsymbol{C}_{ij}
\end{align*}
y como

\item Expansión por cofactores a lo largo de la $j$-sima columna
			\begin{align*}
	\mathrm{det}\, \boldsymbol{\mathrm{A}} &= a_{1j}\, \boldsymbol{C}_{1j} + a_{2j}\, \boldsymbol{C}_{2j} + \ldots + a_{nj} \boldsymbol{C}_{nj} \\
	&= \sum_{j=1}^{n} a_{ij} \boldsymbol{C}_{ij}
\end{align*}
\end{itemize}
\end{bluebox}

\subsection{Propiedades de los determinantes}

\begin{enumerate}
	\begin{bluebox}{Propiedad I}
\item Si una fila o columna de un determinante es cero, el determinante es nulo
\end{bluebox}

Ejemplo:
\[
\begin{vmatrix}
	1 & 2 & 3\\
	0 & 0 & 0\\
	4 & 5 & 6
\end{vmatrix} = \underbrace{\begin{vmatrix}
	0 & 0\\
	4 & 5
\end{vmatrix}}_{0} - 2 \underbrace{\begin{vmatrix}
	0 & 0\\
	4 & 6
\end{vmatrix}}_{0} + 3 \underbrace{\begin{vmatrix}
	0 & 0\\
	4 & 5
\end{vmatrix}}_{0} = 0
\]

\begin{bluebox}{Propiedad II}
\item Si la matriz es diagonal:
\[
	\left( \boldsymbol{\mathrm{A}} \right)_{ij} = A_{ii} \delta_{ij} \implies |\boldsymbol{\mathrm{A}}| = \prod_{i=1}^{N} A_{ii}
\]
el determinante es el producto de los elementos de la diagonal
\[
\boldsymbol{\mathrm{A}} = \begin{bmatrix}
	a_{11} & 0 & \ldots & 0\\
	0 & a_{22} & \ldots & 0\\
	0 & 0 & \ddots & \vdots \\
	0 & 0 & 0 & a_{nn}
\end{bmatrix} \implies \mathrm{det}\, \boldsymbol{\mathrm{A}} = a_{11}\, a_{22}\, \ldots\, a_{nn}
\]
\end{bluebox}

Ejemplo:
\[
\boldsymbol{\mathrm{A}} = \begin{bmatrix}
	1 & 0 & 0\\
	0 & 2 & 0\\
	0 & 0 & 3
\end{bmatrix}
\]
\begin{align*}
	\mathrm{det}\, \boldsymbol{\mathrm{A}} &= \begin{vmatrix}
	2 & 0\\
	0 & 3
\end{vmatrix} - 0 + 0 = 6 \\
		\mathrm{det}\, \boldsymbol{\mathrm{A}} &= A_{11}\, A_{22}\, A_{33} = 1 \cdot 2 \cdot 3 = 6
\end{align*}

\begin{bluebox}{Propiedad III}
\item Si se intercambian dos filas o columnas del determinante, éste cambia de signo.
\end{bluebox}

Ejemplo:
\[
\boldsymbol{\mathrm{A}} = \begin{bmatrix}
	2 & 3 & 5\\
	4 & 8 & 7\\
	9 & 2 & 1
\end{bmatrix} \to \boldsymbol{\mathrm{A'}} = \begin{bmatrix}
	4 & 8 & 7\\
	2 & 3 & 5\\
	9 & 2 & 1
\end{bmatrix}
\]

\[
\mathrm{det}\, \boldsymbol{\mathrm{A}} = -155 \qquad \mathrm{det}\, \boldsymbol{\mathrm{A'}} = 155
\]

\begin{bluebox}{Propiedad IV}
\item El determinante de una matriz $\boldsymbol{\mathrm{A}}$ ha de ser igual al conjugado complejo del determinante de la matriz adjunta de $\boldsymbol{\mathrm{A}}$
\[
	| \boldsymbol{\mathrm{A}} | = \left( | \boldsymbol{\mathrm{A^{\dagger}}}| \right)^*
\]
\end{bluebox}

Ejemplo:
\[
\boldsymbol{\mathrm{A}} = \begin{bmatrix}
	1 + 2i & 5i\\
	3i & 4 + 7i
\end{bmatrix} \to \boldsymbol{\mathrm{A^{\dagger}}} = \begin{bmatrix}
	1 - 2i & -3i\\
	-5i & 4 - 7i
\end{bmatrix}
\]
Calculamos el determinante
\begin{align*}
	|\boldsymbol{\mathrm{A}}| &= \left( 1 + 2i \right) \left( 4 + 7i \right) - 3i \cdot 5i = 5 + 15i \\
	|\boldsymbol{\mathrm{A^{\dagger}}}| &= \left( 1 - 2i \right) \left( 4 - 7i \right) - 5i \cdot 3i = 5 - 15 i
\end{align*}
Siendo $\left( |\boldsymbol{\mathrm{A^{\dagger}}}| \right)^* = \left( 5 - 15i \right)^* = 5 + 15i$
\[
	|\boldsymbol{\mathrm{A}}| = \left( |\boldsymbol{\mathrm{A^{\dagger}}}| \right)^*
\]

\begin{bluebox}{Propiedad V}
\item El determinante de la matriz resultante del producto de dos matrices $\boldsymbol{\mathrm{A}}$ y $\boldsymbol{\mathrm{B}}$ ha de ser igual al producto de los escalares de dichas matrices, ya que el producto de escalares es conmutativo
\[
| \boldsymbol{\mathrm{AB}} | = | \boldsymbol{\mathrm{A}} | | \boldsymbol{\mathrm{B}} | = |\boldsymbol{\mathrm{B}}| |\boldsymbol{\mathrm{A}}|
\]
\end{bluebox}

Ejemplo:
\[
\boldsymbol{\mathrm{A}} = \begin{bmatrix}
	9 & 6 & 3\\
	4 & 2 & 5\\
	7 & 8 & 1
\end{bmatrix} \qquad \boldsymbol{\mathrm{B}} = \begin{bmatrix}
	7 & 4 & 2\\
	0 & 9 & 4\\
	8 & 2 & 5
\end{bmatrix} \qquad \boldsymbol{\mathrm{AB}} = \begin{bmatrix}
	87 & 96 & 57\\
	68 & 44 & 41\\
	57 & 102 & 51
\end{bmatrix}
\]

\[
|\boldsymbol{\mathrm{AB}}| \approx -24786 \qquad | \boldsymbol{\mathrm{A}} | | \boldsymbol{\mathrm{B}} | = |\boldsymbol{\mathrm{B}}| |\boldsymbol{\mathrm{A}}| \approx -102 \cdot 243 \approx -24786
\]

\begin{bluebox}{Propiedad VI}
\item Si dos filas o columnas de la matriz son iguales entre sí, el determinante es nulo.
\end{bluebox}

Ejemplo:
\[
\boldsymbol{\mathrm{A}} = \begin{bmatrix}
	1 & 2 & 3\\
	1 & 4 & 5\\
	1 & 6 & 7
\end{bmatrix} \to \mathrm{det}\, \boldsymbol{\mathrm{A}} = 0
\]

\begin{bluebox}{Propiedad VII}
\item El determinante de la inversa de una matriz es igual a la inversa del determinante de dicha matriz
\[
|\boldsymbol{\mathrm{A^{-1}}}| = |\boldsymbol{\mathrm{A}}|^{-1}
\]
\end{bluebox}

Ejemplo:
\[
\boldsymbol{\mathrm{A}} = \begin{bmatrix}
	8 & 7 & 9\\
	2 & 3 & 6\\
	4 & 1 & 8
\end{bmatrix} \qquad \boldsymbol{\mathrm{A^{-1}}} \approx \frac{1}{110}
\begin{bmatrix}
	18 & -47 & 15\\
	8 & 28 & -30\\
	-10 & 20 & 10
\end{bmatrix}
\]

\[
\mathrm{det}\, \boldsymbol{\mathrm{A^{-1}}} \approx 0.009 \qquad \mathrm{det}\, \boldsymbol{\mathrm{A}} \approx 110 \to \frac{1}{\mathrm{det}\, \boldsymbol{\mathrm{A}}} = 0.009
\]

\begin{bluebox}{Propiedad VIII}
\item Si una matriz es unitaria, entonces
\[
	\boldsymbol{\mathrm{A A^{\dagger}}} = 1 \implies |\boldsymbol{\mathrm{A}}| \left( |\boldsymbol{\mathrm{A}}| \right)^* = \boldsymbol{\mathrm{1}}
\]
\end{bluebox}

Ejemplo:
\[
\boldsymbol{\mathrm{A}} = \begin{bmatrix}
		a & b\\
		-e^{i \varphi} b^* & e^{i \varphi} a^*
	\end{bmatrix} \to \boldsymbol{\mathrm{A^{\dagger}}} = \begin{bmatrix}
		a^* & -e^{-i \varphi} b\\
		b^* & e^{-i \varphi} a
	\end{bmatrix}
\]

\begin{align*}
	\boldsymbol{\mathrm{A A^{\dagger}}} &= \begin{bmatrix}
	a^* a + b^* b & a \left( -e^{-i \varphi} b \right) + b e^{-i \varphi} a \\
	a^* \left( -e^{i \varphi} b^* \right) + b^* e^{i \varphi} a^* & \left( -e^{i \varphi} b^* \right) \left( -e^{i \varphi} b \right) + e^{i \varphi} a^* e^{-i \varphi} a \end{bmatrix}\\
	  &= \begin{bmatrix}
		|a|^2 + |b|^2 & ab \left( e^{-i \varphi} - e^{-i \varphi} \right) \\
		a^* b^* \left( e^{i \varphi} - e^{i \varphi} \right) & |a|^2 + |b|^2
	\end{bmatrix} \\
	  &= \begin{bmatrix}
		|a|^2 + |b|^2 & 0 \\
		0 & |a|^2 + |b|^2
	\end{bmatrix}
\end{align*}

como $|a|^2 + |b|^2 = 1$
\[
\boldsymbol{\mathrm{A A^{\dagger}}} = \begin{bmatrix}
	1 & 0\\
	0 & 1
\end{bmatrix} = \boldsymbol{\mathrm{1}}
\]

Entonces
\[
	\mathrm{det}\, \boldsymbol{\mathrm{A}} = a^* a e^{i \varphi} + b^* b e^{i \varphi} = \left( |a|^2 + |b|^2 \right) e^{i \varphi} = e^{i \varphi}
\]
\[
	\left( |\boldsymbol{\mathrm{A}}| \right)^* = \left( e^{i \varphi} \right)^* = e^{-i \varphi}
\]
\[
	|\boldsymbol{\mathrm{A}}| \left( |\boldsymbol{\mathrm{A}}| \right)^* = e^{i \varphi} e^{-i \varphi} = 1
\]

\begin{bluebox}{Propiedad IX}
\item Si $\boldsymbol{\mathrm{U^{\dagger} O U}} = \boldsymbol{\mathrm{\Omega}}$ y $\boldsymbol{\mathrm{U^{\dagger} U}} = \boldsymbol{\mathrm{U U^{\dagger}}} = 1$ entonces
\[
|\boldsymbol{\mathrm{O}}| = |\boldsymbol{\mathrm{\Omega}}|
\]
ya que
\[
|\boldsymbol{\mathrm{\Omega}}| = |\boldsymbol{\mathrm{U^{\dagger} O U}}| = |\boldsymbol{\mathrm{U^{\dagger}}}| |\boldsymbol{\mathrm{O}}| |\boldsymbol{\mathrm{U}}| = |\boldsymbol{\mathrm{O}}| \underbrace{|\boldsymbol{\mathrm{U^{\dagger}}}| |\boldsymbol{\mathrm{U}}|}_{1} = |\boldsymbol{\mathrm{O}}|
\]
\end{bluebox}

\end{enumerate}

\subsection{Conclusiones}

En este artículo hemos revisado los conceptos básicos de álgebra lineal, allanando el camino para el siguiente documento, que tratará sobre conceptos de álgebra un poco más avanzados con los que empezaremos a tener una base para que el lector pueda empezar a profundizar en temas como la formulación matricial de la mecánica cuántica.

\subsection{Lecturas recomendadas}
Este documento está basado principalmente en el capítulo de repaso de esta materia del libro \textit{Química teórica y computacional}~\cite{bort2001}, ya que es con el libro con el que estuve repasando. Además, para el apartado sobre determinantes y alguna que otra anotación, he usado el libro \textit{Álgebra Lineal. Una Introducción Moderna}~\cite{poole2011algebra}. Recomiendo consultar estos libros si se requiere una explicación más amplia sobre la materia tratada. Por último, también recomiendo el resto de libros de la bibliografía~\cite{rossignoli2018, lay20012algebra} a nivel introductorio, y un libro~\cite{axler2015} un poco más avanzado.

\printbibliography

\end{document}
